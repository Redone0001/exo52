#include <string>

std::string getText(){
    std::string texte = " A day of xenon collisions at CERN\nOn Friday, the Large Hadron Collider at CERN had a day of smashing xenon nuclei together, a departure from its usual diet of protons or lead\nXenon-Xenon collision in the CMS detector at the CERN Large Hadron Collider\n\nThe picture at the top shows what happened in the CMS particle detector when xenon nuclei were circulated in the LHC and brought into head-on collision. The yellow is made up of tracks of electrically-charged particles, produced in such numbers that the whole of the centre of the picture is a yellow blur, with individual tracks only visible near the edges. The blue and green blocks indicate energy deposited by both charged and neutral particles in the CMS calorimeter.\nCollisions between protons look significantly less busy than this, with fewer particles produced. But both xenon and lead nuclei are packed with protons and neutrons, and though lead has more of them, by eye I don’t think anyone could tell the difference between a xenon-xenon collision and a lead-lead one.\nThere are however expected to be differences in detail, on average, in the shape and properties of the exotic ball of material produced in the heart of the collisions.\nBefore it flies apart, this material is a plasma of quarks and gluons, the basic constituents of all nuclear material. Measuring the differences between lead collisions and xenon collisions may teach us more about this strange stuff. If nothing else, it should allow us to make some “control” measurements, a good way of reducing systematic uncertainties. And measuring something new could always throw up a surprise. Time will tell, as the recorded data are carefully analysed.\nCERN makes hotter quark-gluon soup\nRead more\nXenon itself was a “target of opportunity” for the LHC. The noble gas is being injected into the pre-accelerators of the LHC for the benefit of the NA61 experiment, also known as SHINE, the SPS Heavy Ion and Neutrino Experiment (a decent enough acronym by particle physics standards). It was decided to retune the LHC so that, for one day only, xenon could make it all the way into the ALICE, ATLAS, CMS and LHCb experiments too.\nOne of the things SHINE is doing is measuring different nuclear collisions in an attempt to scope out the threshold for actually producing a quark-gluon plasma. To make a plasma, you need a lot of protons and neutrons with a lot of energy each. NA61 aims to find out exactly how many and how much.\nIn a coincidental aside, a xenon nucleus contains 54 protons, and about 77 neutrons on average, which makes its total mass quite close to the mass of the Higgs boson. This coincidence has caused confusion in some circles and I would not be surprised if some theories related to this were to show up in comments below the line.\nThere should be no confusion really. According to the Standard Model, the Higgs boson is infinitely small. We can’t measure “infinitely small”, of course, but from the way it behaves, we can set some kind of upper limit on the physical size of a Higgs. This tells us that if the xenon nucleus were the size of a beachball, the Higgs boson would smaller than the finest grain of sand.\nThis would go some way toward explaining why the Higgs boson was not discovered until 2012, whereas xenon was discovered in 1898. Another reason would be that the Higgs boson rapidly and spontaneously decays into other particles, whilst xenon is stable.\nUnless, of course, you treat it like we did on Friday in the LHC. ";

    texte+="Experiment reveals evidence for a previously unseen behaviour of light\nBeams of light do not, generally speaking, bounce off each other like snooker balls. But at the high energies in the Large Hadron Collider at CERN they have just been observed doing exactly that\nLight is useful, versatile and perplexing. We see with it - that is, we make sense of our surroundings (sometimes) using signals that our eyes send to our brains when light impinges upon them. We also use it to “see” far beyond the limits of our own sense organs. We peer deep into space, or – as at the shiny new light source just opened in Hamburg – into the complexities of atoms and molecules.\nOften we think of light as a wave, and that is how it often behaves. But we know that light comes in little packets – quanta – called photons. The paper which swung the scientific consensus in favour of the existence of photons was published by Albert Einstein in 1905, and yet in 1951 he wrote to an old friend\nThese fifty years of conscious brooding have not brought me any closer to an answer to the question: “What are light quanta?” Today the first rascal believes he knows what they are, but he deludes himself. (Albert Einstein to Michele Besso, December 12, 1951; Einstein and Besso 1972, 453)\nWhatever Einstein meant by “answering the question”, I’d argue that in the fifty years before he wrote that we certainly did get closer, and in the 66 years since we have got closer still. Perhaps that makes me a rascal, but I’m definitely not the first.\nI think we get closer to answering the question of what photons are when we observe and understand new forms of behaviour they exhibit. This is a pretty general approach among physicists; what something “is” is defined by its properties.\nPhotons exhibit an incredibly wide range of behaviour, depending upon their wavelength, how densely they are packed together, what materials they are interacting with and what direction they are spinning. Lasers and optical fibre technologies are just two of the more obvious applications that have had an impact on our lives.\nThis phenomenon is impossible in classical theories of electromagnetism\nDan Tovey (University of Sheffield), ATLAS Physics Coordinator.\nSome of my first work as a postdoc involved measuring how photons scatter off protons, and we even wrote a programme which could simulate how one photon collides with another producing sprays of other particles. When things break up like that in a particle collision, we call it “inelastic” scattering.\nA much more elusive process is the elastic scattering of photons; light bouncing off light, and nothing else being produced. The term “elastic” is used because, as when bouncing a rubber ball, the amount of kinetic energy – the energy of motion of the ball – is conserved. The ball bounces back pretty much as hard as you throw it. (In an inelastic collision some of this energy is distributed amongst the new particles, or lost as heat, or otherwise converted into a different form.)\nThe first strong evidence for the elastic scattering of photons at high energies has just been reported by the ATLAS experiment (which I work on) at the Large Hadron Collider.\nThe measurement of this process is made during the periods when the LHC is smashing lead nuclei together, rather than in its usual proton-smashing mode. Because the lead nucleus has a charge 82 times that of the proton, and because electric charge is a source of photons, there is a dense cloud of high-energy photons accompanying the bunches¹ of nuclei. Sometimes nuclei hit each other, sometimes they hit photons, and sometimes – more rarely – photons hit each other and bounce off elastically.\nThe analysis involves identifying photons in the ATLAS detector in collision events which are otherwise very quiet. The picture at the top of the article shows a representation of the real data from once such collision. The beams come from behind and in front of the picture and collide in the centre of the blue and purple circles. Those circles are the charged-particle detectors of ATLAS, showing almost no activity, because photons carry no electric charge. The green and yellow wedges show the energy deposited by the scattered photons in the calorimeter – the part of ATLAS responsible for energy measurement. A more usual lead-lead collision would be a bit busier, something like this:\nOne of the first higher energy (13 TeV) heavy ion collisions recorded by the ALICE detector, on 25 November 2015\nOne of the first higher energy (13 TeV) heavy ion collisions recorded by the ALICE detector, on 25 November 2015 Photograph: ALICE/CERN\nElastic light-light scattering is an intrinsically quantum-mechanical process. The fact that it occurs is not unexpected; it makes an essential contribution to the magnetic moment of fundamental particles, for example, and closely-related processes have been observed at lower energies². But this is the first time two high-energy beams of light have been seen scattering off each other.\nAs Dan Tovey says “this result provides a sensitive test of our understanding of QED, the quantum theory of electromagnetism.” Or to put it another way; experiment, rather than conscious brooding, has brought us a step closer to answering Einstein’s question. Light quanta are things that do this, too.";

    texte+=" Getting to the bottom of the Higgs boson\nAs the Large Hadron Collider at CERN continues probing the high-energy frontier of physics, a new feature of its greatest discovery so far has come into view\nATLAS data on the decay of the Higgs boson to bottom quarks\nIn high-energy particle collisions we study the smallest known constituents of matter. According to our best knowledge of physics, these constituents have mass only because of the way they interact with a unique quantity which permeates all of space. This quantity, like practically everything else in the strange world of the very small, is a quantum field.\nSo much for the recap. Last week we learned something new about the Higgs boson\nThat is not what makes this quantity unique. Quantum fields are all over the place. The light by which you are reading this text is a wiggling quantum field (an electromagnetic field, in that case). What makes the field involved in giving mass to particles unique is precisely the fact that it exists everywhere. It is present even in the lowest energy, emptiest vacuum. In fact, unlike any other quantum field, if you wanted to get rid of this field in a region of space, you would have to add energy, not remove it.\nThe wiggles in this mass-endowing field are called Higgs bosons, after one of the three theorists (Brout and Englert being the other two) who first postulated this theory, in the early 1960s. Five years ago, in 2012, the Higgs boson was discovered in high-energy collisions of protons in the Large Hadron Collider (LHC) at CERN, demonstrating that the essentials of this theory of mass were correct.\nSo much for the recap. Last week we learned something new about the Higgs boson.\nWhen a Higgs boson is made, it very rapidly falls apart; that is, it decays. The fact that it was ever there at all is determined by measuring the fragments emitted in these decays. The boson has various decay options open to it, and for any individual Higgs, the option it takes is decided by the rolling dice of quantum mechanics. Our best theory – the Standard Model of particle physics – predicts the probability of each choice.\nHiggs-like discovery from the inside\nThe prediction is that the most popular choice will be a decay to bottom quarks. Quarks are the particles which make up protons and neutrons, which make up the nucleus of every atom. The bottom quark is the most massive quark to which the Higgs can decay – which, because of the role the Higgs plays in mass, is the reason for its popularity. The theory predicts that 58% of Higgs bosons will produce a bottom quark and anti-quark when they decay.\nDetermining whether or not this prediction is correct is a very important task, and a surprisingly difficult one. It is important in that the answer would either provide another pillar of support for the Standard Model understanding of particle masses, or fatally undermine it. It is difficult because there are many, many ways of producing bottom quarks at the LHC which have nothing much to do with the Higgs. The background ‘noise’ is huge, which is why the Higgs was initially discovered via other decay products, which are rarer but which have less background noise.\nDigging out a signal for Higgs bosons decaying to bottom quarks therefore requires applying complex selection filters on the millions of collisions measured at the LHC. Every one of these filters could go wrong, could bias your result, so the analysers practice “in the dark” for a long time, using simulated data, and real data from similar collisions, to cross-check and calibrate without looking at the place they actually expect the signal to be; this is a technique called a “blinded analysis”.\nHiggs boson: the definite article?\nAs you might imagine, the moment at which you decide your analysis is as good as you can get it, and “unblind” yourself to see whether the signal is there, is quite tense. Tim Scanlon at UCL is one of the leaders of the analysis team. As he puts it:\nAfter 5 years of searching for H->bb, I spent an anxious couple of hours waiting for the results after asking one of our key analysers (Andy Bell, a PhD student) to unblind the result. Would we see what we’ve been waiting for over the past five years? Would we be unlucky and just fall short? Or despite all our best efforts to ensure the analysis was correct, could we have missed something critical and have produced a seriously flawed result? Even if you are confident that your analysis is solid, you are still relying upon the roll of a dice when on the threshold of discovery.\nATLAS data on the decay of the HIggs boson to bottom quarks\nATLAS data on the decay of the Higgs boson to bottom quarks. The black points are the data, the grey area shows the expected background and the red shows the amount of Higgs signal which best agrees with the data. The horizontal axis is the measured mass of bottom quark-antiquark pairs. The probability of the background producing a distribution like the data, without adding in the red Higgs contribution, is less that 0.0023. Photograph: ATLAS/CERN\nThe result, shown in the plot, was positive – significant enough evidence to supersede the hints from previous experiments and to cross the (arbitrary but important) “three sigma” threshold, meaning that if the data were just random noise, without a Higgs, they would produce a result like this (or stronger) less than 0.23% of the time. As Tim puts it\nThis is the accumulation of the work of a lot of people over the last six years, where each new result has built upon the last.\nImportant physics at the LHC continues, long after the big splash of the first Higgs discovery. Step by step, we are learning more about how the universe we inhabit operates.\n";
    
    texte+="Newly discovered particles, and what's in them\nQuarks, basically. But more charming than usual\nRepresentation of a doubly heavy-quark baryon, such as that discovered at LHCb Photograph: Daniel Dominguez/CERN\nLast month the LHCb experiment, at CERN’s Large Hadron Collider (LHC), reported the discovery of a new particle. While this received a reasonable amount of attention, it didn’t really cause as much excitement as, say, last year’s unconfirmed hints of a new particle from the ATLAS and CMS experiments (also at the LHC), which turned out in the end to be just a statistical glitch.\nThose hints evaporated when more data came in, as such glitches do. The new particle at LHCb has passed a much higher statistical threshold, and seems to be here to stay. The animation here shows how the signal developed in the data over time as the LHCb experiment recorded and analysed more data. The peak indicating the presence of the new particle, call a Ξcc++ (pronounced Ksi c c plus plus) is pretty convincing by eye, an impression confirmed by solid statistical analysis. So why isn’t there (even) more excitement about this particle? There are good reasons, and they are worth looking into.\nAerial photo of CERN showing the path of the LHC, and the ALICE, ATLAS, CMS and LHCb experiments.\nAerial photo of CERN showing the path of the LHC, and the ALICE, ATLAS, CMS and LHCb experiments. Each experiment is a particle detector, surrounding one of the positions at which the LHC beams collide. Photograph: Maximilien Brice/CERN\nThe main reason is that the new particle is not what a particle physicist would call ‘fundamental’. In this context that means that it is made up of smaller particles bound together. Those smaller particles are quarks in this case, meaning that the Ξcc++ is a hadron – the generic name for particles made of quarks. The most common hadrons are the proton and the neutron, which are the constituent parts of atomic nuclei. There are known to be many more hadrons too – all of them made of quarks and/or antiquarks.\nThe current “Standard Model” of particle physics allows for no new fundamental particles, but it does allow for new hadrons – as long as they are made up of the quarks we already know. This means that a new fundamental particle would change our view of physics in a way that a new hadron does not.\nFurthermore, there are some outstanding problems with the Standard Model which give good reasons to hope that additional fundamental particles exist; the discovery of such a particle could be a breakthrough in addressing these questions. The usual ‘top three’ would probably be what is Dark Matter, why is there so much more matter than antimatter around, and how does gravity fit in? Generally, physicists don’t expect additional hadrons to help much with any of that.\nPhysicists may of course be wrong.\nFor example, in the Standard Model the strong force, which is responsible for binding quarks together to form hadrons, treats matter and antimatter equally. But this equal treatment is imposed rather arbitrarily. It would be very easy to allow some asymmetry here which could help explain the observed preponderance of matter over antimatter in the universe; from some points of view, this would in fact be more natural. It just hasn’t been observed so far.\nWe don’t expect to observe the strong force playing favourites in this new hadron, but expectations can be confounded. And this is a new form of hadron – the first seen containing two charm quarks. Maybe some subtle differences in behaviour could be revealed by studying it.\nWhile we’re speculating, it is worth pointing out that some physicists have proposed that Dark Matter may well be some exotic form of hadronic material.\nNeither of these possibilities is really seen as a good bet by most theorists, but it is the job of experiment to make measurements and observations of new phenomena. This hadron is certainly new.\nThe strong force is responsible for most of the mass of atoms and molecules, it dominates the interactions between particles at the high energies seen in stars, supernovae and cosmic rays. I have a feeling that understanding the strong force, and the behaviours that emerge from it, is a somewhat underrated endeavour in physics.\nAfter months of excitement, we're left with the status quo. But it's no bad thing in physics\nThe trouble is that is a very difficult problem. We have a lot of data on hadrons already, and it is not clear how discovering more of them will help much. Making predictions using the strong force is very challenging, even when only a few quarks are involved. For more measurements to lead to a leap in understanding, we will probably need a parallel leap in theoretical inventiveness or precision.\nIn the meantime, perhaps we set our expectations too high. LHCb is collecting data on a region of nature to which we have never before had access. Discovering a new planet orbiting a distant star doesn’t necessarily revolutionise our understanding of astrophysics – but is it still exciting and important. Maybe that’s the best way to look at the Ξcc++ . Guy Wilkinson of Oxford University, who led LHCb while these data were being taken, said of the new discovery that, in contrast to other hadrons containing three quarks in which the three quarks perform an elaborate dance around each other,\n    ... a doubly heavy baryon [such as Ξcc++] is expected to act like a planetary system, where the two heavy quarks play the role of heavy stars orbiting one around the other, with the lighter quark orbiting around this binary system\nIt is right that we take delight in finding such an object.";

    texte+="Totally stuffed: Cern's electrocuted weasel to go on display\nStone marten, which met its fate at the Large Hadron Collider, to become part of Rotterdam museum’s exhibition on ill-fated human-animal interactions\nThe stone marten met its fate when it hopped over a substation fence at the Large Hadron Collider (LHC) and was instantly electrocuted by an 18,000 volt transformer.\nA stone marten (not pictured above) met an unfortunate end when it hopped over a substation fence at the Large Hadron Collider (LHC) and was electrocuted. Photograph: Alamy\nThe singed fur and charred feet are testament to the weasel’s last stand: an encounter with the world’s most powerful machine that was never going to end well.\nNow an exhibit at the Rotterdam Natural History Museum, the stone marten met its fate when it hopped over a substation fence at the Large Hadron Collider (LHC) near Geneva and was instantly electrocuted by an 18,000 volt transformer.\nThe incident in November last year knocked out the power to the vast particle accelerator which recreates in microcosm the primordial fire that prevailed at the birth of the universe. The partly-cooked corpse was duly secured for inclusion in the museum’s Dead Animal Tales exhibition.\n“It’s a fine example of what the exhibition is all about,” said Kees Moeliker, director of the museum. “It shows that animal and human life collide more and more, with dramatic results for both.”\nThe Cern stone marten, secured for inclusion in the Rotterdam Natural History Museum’s Dead Animal Tales exhibition.\nThe stone marten is the latest dead animal to go on display at the museum. It joins a sparrow that was shot after it sabotaged a world record attempt by knocking over 23,000 dominoes; a hedgehog that got fatally stuck in a McDonalds McFlurry pot, and a catfish that fell victim to a group of men in the Netherlands who developed a tradition for drinking vast amounts of beer and swallowing fish from their aquarium. The catfish turned out to be armoured, and on being swallowed raised its spines. The defence did not save the fish, but it put the 28-year-old man who tried to swallow it in intensive care for a week.\nIt was another unfortunate incident that spurred Moeliker to establish the exhibition in the first place. In 1995, a male duck flew into the glass facade of the museum and died on impact, a fate that did not deter another male duck from raping the corpse for 75 minutes. The incident ruffled feathers in the community but earned Moeliker a much-coveted IgNobel prize when he published his observations . “I was the one and only witness,” Moeliker said. “I’m a trained biologist but what I saw was completely new to me.”\nThe LHC has been brought to its knees by stone martens before. In April last year, one of the animals bounded into a 66,000 volt transformer and shut the collider down for a week. The Rotterdam museum tried to obtain the remains of the beast, known as the “Cern weasel”, but the highly efficient staff at the European particle physics laboratory had already disposed of the corpse. When a second stone marten met a similar fate in November, Moeliker was ready to secure the animal for the exhibition.\nStone martens - or “fouines” - have a habit of gnawing through electrical cables and are known for causing power outages in the region. “We want to show that no matter what we do to the environment, to the natural world, the impact of nature will always be there,” Moeliker said. “We try to put a magnifying glass on some fine examples. This poor creature literally collided with the largest machine in the world, where physicists collide particles every day. It’s poetic, in my opinion, what happened there .";
    texte+="Is the Standard Model isolated? | Science\nIn CERN, on the outskirts of Geneva, preparations are well underway for the next spate of particle collisions at the Large Hadron Collider (LHC). We are planning for a record year, starting in June. Last year was a bounteous one, but in 2017 we expect even more collisions, at the same record-breaking high energy.\nThe detectors which will record the data have been under maintenance and refurbishment. Most notably, the vertex detector in the centre of the CMS experiment has been entirely replaced. The vertex detector is made of silicon; the electrons in the material only need a small nudge from a passing charged-particle to escape and carry an electric current. Those tiny currents allow us to track the path of the particle, and thus work out where it originated – the vertex. The vertex detector is a vital and complex component of the experiment. You can see a video of the (very careful) operation to insert the new one, below.\nA new vertex detector installing in the centre of the CMS detector on the LHC at CERM, March 2017\nThe LHC revealed the Higgs boson in 2012, but we have made no other major discovery since. It is worth asking what we hope to learn from the data coming soon.\nThe discovery of the Higgs boson established that the current theory, the ‘Standard Model’ can potentially work up to very high energies – as high the LHC can probe and beyond. High energies also correspond to short distances, so we are looking at the tiniest, most fundamental constituents of the universe.\n    the LHC invites us to consider the possibility that the Standard Model is isolated from new physics\nThere is a key energy scale in nature, in the region of the Higgs mass, which we call the ‘electroweak symmetry-breaking scale’. At this scale, the masses of fundamental particles originate. Above this scale, the weak and the electromagnetic fundamental forces come together. Physics looks very different, and without the Higgs we would have no fundamental understanding of it.\nWith the Higgs now established, the Standard Model makes definite predictions for the physics we should see in this new territory, and measuring precisely whether those predictions work should be the most important item on the ‘to do’ list with the new data. With more data, we will reach higher precision and open up new, rare and previously unobserved processes.\nThe Standard Model may be all we need, and all we see, at the LHC. However, there are good reasons to think, and hope, otherwise.\nPowerful though it is, the Standard Model is no ‘Theory of Everything’. It does not incorporate gravity, it does not contain an explanation for the preponderence of matter over antimatter that we see around us, and it does not contain a convincing candidate for the ‘Dark Matter’ needed to explain the rotational speeds of stars in galaxies and other astrophysical observations. And it has nothing much to say about the ‘Dark Energy’ to which we ascribe the accelerating expansion of the universe.\nThat is quite a to do list, and we will be searching at these high energies for clues to these problems and omissions. With more data, we will have greater sensitivity to the various extensions to the Standard Model suggested by theorists to address these open questions. It is certain that we will rule out many of their suggestions. It is possible we will rule them all out, and the Standard Model will triumph again.\nSuch a scenario would put us in a new situation, which is framed very well in a recent article by James Wells, Zhengkang Zhang and Yue Zhao (an article by James has featured here before, he seems to have a knack of provoking me to write). If nothing beyond the Standard Model shows up at the LHC, we will in a sense have established that the Standard Model is isolated.\nWells et al offer a definition of what this means, and it is necessarily a little arbitrary, but the sense of it is clear. We have a richly-populated landscape of physics up to the electroweak symmetry-breaking scale, but then a gap. A desert, or perhaps an ocean, with no new particles or forces but those of the Standard Model. The Standard Model will be doing its thing in interesting and previously unobserved new ways, which will be worth studying, but unless we have underestimated it badly, this will not address the open questions – Dark Matter and so on – listed above.\nLike the distances involved in space exploration, the scale of the search is inhuman. The LHC can probe structures about a hundred-million times smaller than an atom. The scale at which problems with quantum gravity definitely become unavoidable is about 1017 (one followed by 17 zeros) smaller still. There are good reasons to think that this gap is not entirely empty, but it is still a vast gulf beyond our current reach, in which new physics may hide.\nThe LHC is not the only player in this, of course. Ultra-precise measurements of rare processes at lower energy, the behaviour of neutrinos (still responsible for the only major change in the Standard Model since its inception), astrophysical observations, or other innovative experiments not yet thought of – any or all of these may throw up clues as to whether the Standard Model is indeed isolated, and by how much, if so. But the data from the LHC this year and next will be a major landmark on the way.";
    texte+="A new CERN experiment targets even higher energies (eventually) | Science\nHigh-energy particle beams have multiple uses. In general, a controlled beam of high-energy particles can be used to smash things up – for example tumours or protons – or study them at high resolution, revealing the structure of molecules and materials, or indeed new fundamental physics such as the Higgs boson.\nThe challenges involved in producing a useful beam are various, and depend on the type of particle and what you want to use the beam for. One common limitation is the size and cost of the machine, and another is the steepness of the accelerating voltage gradient.\nThe accelerating voltage in a standard battery is 12 Volts. Mains power could get you 240 V. But many applications require millions or even billions of times this. The Large Hadron Collider (LHC), at the current frontier of high-energy physics, accelerates particles to 13 TeV, more than a million million times the energy that can be reached using the batteries hopefully included in your Christmas presents.\nThe voltage gradient is a measure of how much voltage can be applied over a certain distance. For many accelerators¹ this is what determines the size. Can you get from zero to a billion in a few centimetres, metres, or kilometres?\nOver the last few years, several experiments have shown that very high gradients can be produced in plasmas. A plasma is a gaseous mixture of positively and negatively charged particles, usually produced at high temperatures. Old neon lamps contain plasma.\nIn normal circumstances, the positively and negatively charged particles in a plasma are evenly distributed. But firing a laser, or a beam of particles, into a plasma, disrupts this, producing regions in the plasma of strong net positive charge, and others of strong negative charge. The gradients produced between these regions, in the wake of the beam, can be fantastically strong, and can be used to accelerate particles.\nSuch acceleration has already been demonstrated using lasers and electron beams injected into plasma, and these offer the possibility of accelerators for medical and other applications that are much smaller and cheaper than any currently available. The goal of the Advanced Wakefield Experiment (AWAKE) at CERN² is to demonstrate plasma wakefield acceleration using a proton beam.\nProton beams are available at higher energies than lasers or electrons, and the most energetic are at CERN. AWAKE is using protons from the Super Proton Synchrotron (SPS), the final booster stage in the CERN accelerator complex before the LHC (see diagram).\nThe eventual aim of AWAKE is to demonstrate the feasibility of using such a high-energy proton beam in a plasma to accelerate an electron beam to higher energies than ever before.\nIt is possible you have heard of AWAKE because when beams first entered the experiment in June last year, there were some wild headlines in news outlets (such as the Express and The Sun), along the lines of CERN opening ‘portals’. The spurious excitement seems to have been caused by some pictures of a thunderstorm. Thunderstorms are really, really rare around the mountains of central Europe, and a particle physics experiment passing a milestone on schedule is even more unusual, so you can see how the journalists involved could have got confused.\nAnyway, that was an important step forward, and at the end of LHC running in 2016, another major step was taken. The experiment for the first time measured the fact that the shape of the proton beam was being modulated, or shaped, by the plasma. This is a sign that indeed the desired very high gradient electric fields are being produced in the plasma – the first time this has been seen for a proton beam.\nThere is some more information, and a nice 360-degree view of the experiment, in this CERN news article. There is a long way to go, but 2016 was a good year for protons at CERN, and not just in the LHC. \nHow to hide a 'fifth force' – and how to find one | Science\nEinstein’s General Relativity provides an elegant description of how space, time and matter affect one another. It makes precise predictions of gravitational effects, which have been verified by many measurements.\nBut if we use the theory to try to understand the motion of galaxies, we get the wrong answer, unless we invent a new form of so-called ‘dark’ matter. This is not a small correction – there needs to be much more of the Dark Matter than normal matter, and what is more, it doesn’t seem to be made up of quarks and electrons like all other matter. In fact it doesn’t seem to be made up of any of the particles in the Standard Model of particle physics.\nFurthermore, the universe is expanding at an increasing rate, rather than – as you might expect if it started of with a big bang and then gravity takes over – slowing down. This effect we ascribe to something we call ‘Dark Energy’. Dark Energy can be accommodated within General Relativity, but only by adding an absurdly precise ‘cosmological constant’, which looks very weird, or “unnatural” as a theoretical physicist would put it. the first plausible theory I have come across in which the LHC can to contribute to understanding Dark Energy\nIt gets worse. Dark Energy is a sort of ‘energy in empty space’, and particle physics also predicts this kind of energy, due to quantum fluctuations of the Standard Model – including and especially the Higgs boson. But these fluctuations would naively lead to so much Dark Energy that atoms themselves (never mind theoretical physicists) would never form in the first place.\nThat’s wrong, obviously.\nAnd then there’s the nagging, possibly connected, fact that we don’t have a way of making General Relativity and quantum theory work together at very high energies.\nSince gravity is a common thread here, all of these problems might seem to imply that General Relativity needs to be modified in some way. That’s a thought that has occurred to many physicists. However, General Relativity is so subtle, and so, well, General, that replacing it, or even successfully tweaking it, is a very hard thing to do.\nStill, physicists are persistent, and there are new ideas coming forward all the time. One possible tweak is to postulate a new particle which carries a ‘fifth force’ (the other four forces being electromagnetism, the weak and strong interactions, and gravity).\nConsider a pointillist painting. At short distances, dots. Further away, a picture. Further away still, a dot again.\nTo explain Dark Energy, this force has to affect all matter – as gravity itself does – and operate over large distances. Such forces have been looked for already, and if they affect the motion of the planets in the solar system, for example, they have to be enormously more feeble than gravity, otherwise we would have seen them already. But if they are enormously more feeble than the gravity between stars and galaxies, they won’t make any difference to the Dark Energy or Dark Matter problems, so that’s a waste of time.\nOne way potential way around this conundrum is a process called ‘screening’, in which the strength of a force depends upon the environment it is in. A recent paper from a group at the University of Nottingham describes a model in which the force is screened by matter itself. In dense regions of the universe (like the Earth, for instance) the force is hidden, while in empty space, the force can operate. In the case of the Dark Energy problem, which is what the theory was aiming for, this can provide exactly what the data need. The force can make the universe accelerate at large distances, while having no measurable effect on the orbit of the planets. As a bonus, this new force can also have a significant impact on the way galaxies rotate, which might at least partially solve the dark matter issue as well.\nTo a physicist, the way this new force works is reminiscent of the way the theory of Brout, Englert and Higgs gives mass to fundamental particles. It involves a scalar boson – a particle like the Higgs boson, which has no spin – and it involves the idea of symmetry breaking¹. But I am aware that using the Higgs as an analogy to explain something to a general audience is not a winning strategy, so here’s a better attempt, I hope. Consider a pointillist painting:\nAt short distances, dots. Further away, a picture – and the dots are hidden. Further away still, and the picture is a single dot again.\nPaul Signac Femmes au puits 1892 (détail couleur) Photograph: By Piscis13 [Public domain], via Wikimedia Commons\nWhen averaged over dense regions of space, a symmetry hides the fifth force. This is like looking at the picture above from a metre or so away. The dots are hidden in the colours and landscape of the painting.\nClose to the painting, the dots are visible. In the same way, for things the size of atoms or smaller, the averaging doesn’t happen, so the fifth force may show up.\nAnd on very long distance scales, space is empty, so the density is low and again the force reappears. Similarly, a very long way from the painting, it is just a single dot again.\nAll this is speculative, of course. Speculative ideas are one thing we expect from theorists. Another thing we expect is testable predictions, and this model seems to be testable in an excitingly wide range of experiments. Upcoming observatories, such as the European Space Agency EUCLID mission and the Dark Energy Spectroscopic Instrument, will characterise gravity and dark energy on astrophysical scales. Precise atomic physics experiments could measure the effect of the fifth force on atoms, and most interestingly to me personally (since I work on it), this is the first plausible theory I have come across in which the Large Hadron Collider can contribute to the understanding Dark Energy.\nThink of the universe as a skateboard park: Supernovas and sphalerons | Science\nLast week I spent a couple of days in Abingdon discussing particle physics, and specifically where we might be headed with the high-energy collision data from the Large Hadron Collider (LHC) at CERN. Whatever it may be remembered for in terms of celebrity deaths and politics, 2016 has been a great year for the LHC, with more collision data delivered than ever before, at the highest energy so far.\nI shared a taxi from Didcot Parkway with John Ellis and Nick Mavromatos, two particle theorists from King’s College London (though John has spent much of his career at CERN). Whenever two or more scientists get together these days, they will discuss Brexit and/or Trump. But (probably desperate to change topic) we also talked about some relatively recent evidence for pieces of supernova impacting the Earth about 2.5 million years ago. This article is written in a similar spirit.\nThe supernova evidence was reported on by Tim Radford in April, but John and colleagues wrote a paper in 1996 suggesting the method that was used, and he has given talks on the whole area recently.\nIt seems supernovas aren’t enough. Nature has an even more extreme way of making heavy elements. And we may be on the point of observing the distortions of space-time caused in the process\nIt is a marvellous fact that the heavy elements on Earth were all created in violent astrophysical cataclysms, but the story of this supernova is especially specific, because the actual atoms can be dated (at least statistically). An isotope of iron, Fe60, was measured at a particular depth in the ocean’s crust. The position in the crust sets the date, and the presence of the isotope betrays the supernova.\nSphalerons are very exotic, but they are a prediction of the Standard Model of particle physics\nThe word “isotope” comes from the Greek, “iso” meaning the same, and “topos” meaning place, because different isotopes of the same element occupy the same place in the periodic table. Atomic nuclei contain protons, with positive charge, and neutrons, which are neutral. Isotopes of an element differ from each other only by the number of neutrons. This has no effect on the chemical properties of the element, because it is the number of protons that determines the number electrons that must bind to it in a neutral atom, and the electrons in turn determine the chemistry. Hence the same slot in the periodic table.\nThe mass will be different though, and this can, with care, be measured. Also, since the neutrons are important in holding the nucleus together, many isotopes are unstable, and decay to lighter elements over times ranging from fractions of a second, to millions of years.\nIron has 26 protons, and the most stable, and therefore most common, isotopes of iron have between 28 and 32 neutrons. The “60” in Fe60 is there because it has 34 neutrons, making the total of neutrons plus protons equal to sixty – the atomic mass.\nIf you have a sample of Fe60, after 2.6 million years, half of it will have decayed. Since the Earth is about 4.5 billion years old, any Fe60 it might have started with has long gone, and the abundances measured in a specific layer of the ocean crust can only have come from outer space. And they match the predictions Ellis and others made for delivery from a nearby (by astronomical standards) supernova, one which would have been easily visible, and perhaps as bright as the moon.\nBy particle physics standards, twenty years (1996 to 2016) between a prediction and a measurement isn’t too bad. The Higgs boson took nearly 50 years, but the impact on physics of that discovery is still being felt. The meeting in Abingdon was full of examples of this impact, and one of them highlights another connection between things we can hold in our hands and potentially mind-blowing physics, over timescales much longer than 50 years, or even the millions of years since the supernova.\nThis connection involves weird objects called sphalerons, and probably explains not just how the chemical elements got here, but how the particles they are made of – electrons, and the quarks inside the protons and neutrons – came to be. Every time I hear about them, I need to have sphalerons explained to me again, which is a sign that I don’t thoroughly understand them. But it goes something like this.\nSphalerons are very exotic, but they are a prediction of the Standard Model of particle physics, not something beyond it. And now we know the Higgs exists, we know they should exist too¹.\nTo get an idea of what a sphaleron is, you have to think of the way we describe particles, using something called ‘perturbation theory’. Try this. Imagine the empty universe, with no energy, sitting at the bottom some of some valley, say on a skateboard park.\nAdding a bit of energy corresponds to allowing skateboarders to zip up and down the sides. If the energy is small, they will roll back down again, go up the other side, and oscillate like that. These oscillations correspond to particles in perturbation theory, rippling along through space-time in their merry quantum way.\nBut if you add a lot of energy, the skateboarder can zip over the wall of the valley, into the next door valley on the park. That’s a sphaleron, sort of.\nThey would have been around in the early universe, because the energy density was very high then. And they seem to have played a crucial role, as follows: There are various things which don’t change in perturbation theory, things that are conserved. One thing, in our skateboard universe, is the average position of the boarders. They spend as much time on one side of the valley as they do on the other side; the average is the bottom of the valley.\nBut if a boarder sphalerons his or her way over the wall into the next valley, their average position has moved – to the centre of the next valley. The conservation law has been violated. Sphalerons in the early universe violate conservation laws too, and one that they violate is the number of particles². They can add more quarks and leptons to the universe. This is an essential part of how the stuff we are made of got here at all.\nSo not only are many of the heavier atomic nuclei around us made in supernovae, the electrons orbiting them are the result of sphalerons.\nI’m aware that “think of the universe as a skateboard park” is a a bit Douglas Adams, but it’s the best I can do. And in difficult times, this kind of interconnectedness can be strangely comforting.\nCould the Higgs boson have been discovered by accident? | Science\nWe have a tendency to oversimplify complicated issues. Sometimes this gives useful clarity, but more frequently it gives a distorted impression of what I am stubborn enough to call the truth. Clarity can be seductive, but is disastrously misleading if it neglects important facts. This is true in politics, in history, and in science.\nIn a recent article on how we came to discover the Higgs boson, my colleague, theoretical physicist James Wells, puts it like this:\nA terse and deleteriously incomplete history of the Higgs boson says that it was postulated in 1964 by the theorist Peter Higgs and then discovered in 2012 by experimentalists after a multi-decade herculean construction project at CERN to find it.\nThis is, he says, a distortion of how science progresses, and of what enables scientific discoveries.\nThe Higgs boson was the last particle of the ‘Standard Model’ of physics to be discovered. It occupies a unique and essential place in that theory, and its discovery validated our understanding of how fundamental particles can have mass. You might think the history of that discovery would be thoroughly understood and agreed on, but Wells detects a false and seductive mythology: a ‘Eureka moment’ for Higgs in 1964 followed by 48 years of experimental labour until the triumphant announcement by the ATLAS and CMS collaborations on 4 July 2012.\nHis article is a good read for anyone unafraid of an equation or two, and his main point is that elucidating the observable consequences of the idea conceived by Higgs (and Brout and Englert) in 1964 required a series of major theoretical advances over the intervening years, and an amount of hard graft at least comparable to that involved in building the experiments. The discovery papers of ATLAS and CMS cite 115 theory papers. Wells lists them in his article. They cover the original ideas, understanding their appearance in a particle collision, what other physics might fake that appearance, what the theoretical uncertainties are, and more. ATLAS and CMS could have cited many more papers, but you have to draw the line somewhere.\nThe point is convincingly made, and should be taken on board by anyone tempted by the simplistic ‘lone theorist hero and herculean experimentalists’ mythology.\nWells makes another point, however, with which I disagree. He says that the theory input is so important that\nThe Higgs boson could not have been discovered experimentally by accident.\nI think it could.\nThe Higgs boson is produced when particles are brought into collision with each other at high energies, as was done at the Large Hadron Collider at CERN. Once produced, the Higgs decays rapidly to other particles, and digging the signs of this decay out of all the other particles produced in the collisions is a large part of the experimental challenge. The Higgs can decay several different ways, and some of them would surely not have been untangled any time soon without a lot of guidance from theoretical predictions such as those described by Wells.\nBut one of the ways the Higgs can decay is rather striking and is not overwhelmed by fake ‘backgrounds”. This is the decay to four leptons (either electrons, muons or their antiparticles). Any experiment at a high energy collider would look for four leptons and measure their mass. In fact this is the measurement I described just last week. The bump at 125 GeV in that ‘plot with the most physics’ is pretty clear - we would, I am confident, have made that plot and seen that bump, even if we hadn’t been looking for the Higgs.\nA long-ish read about a plot with lots of physics. Not the plot of a film or a novel but, you know, a plot!\nNow it is true that without the theory we might never have built the collider, or ATLAS and CMS. Counterfactual history is tricky. But there are good general reasons for building high-energy colliders – they allow us to study the smallest constituents of matter, and we knew that was the case already without help from Higgs. So I think high energy physics would probably have got there in the end.\nWhat is certainly true is that without the theory we would have been slower, and would not have known immediately what that bump was. We’d have been excited about it, and there would probably have been a lot of theories produced very quickly, as there were when we thought we might have another one at 750 GeV. Maybe that is what Wells means – without the theory, we could not have understood the implications of the bump, or connected it to the origin of particle masses. In that case I would agree with him.\nBut the Higgs boson could, and I think probably would, have turned up by accident, nevertheless. And the theorists would have told us what it meant, eventually.";

    
    
    return (texte);
}

